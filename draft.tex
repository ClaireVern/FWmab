\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amssymb}
\usepackage{babel}


\begin{document}

\title{A bandit framework to speed up Franck-Wolfe}


\author{Aurélien Bellet, Michal Valko, Claire Vernade}

\maketitle
The objective is to solve the following convex optimization problem
given a convex set of constraints $\mathcal{D}\in\mathbb{R}^{d}$:

\[
\min_{x\in\mathcal{D}}f(x)=\sum_{i=1}^{n}f_{i}(x).
\]
For that task, we suggest to use Franck-Wolfe algorithm that proceeds
in two steps : start with $x^{(0)}$ and iterate for $k=0,1,...$
\begin{enumerate}
\item $s^{(k)}=\arg\min_{s\in D}\left\langle s,\nabla f(x^{(k)})\right\rangle $,
(direction finding),
\item $x^{(k+1)}=(1-\gamma_{k})x^{(k)}+\gamma_{k}s^{(k)}$ , for a well
chosen $\gamma_{k}$ in order to guarantee convergence.
\end{enumerate}
At each iteration, for the direction finding step, the algorithm is
supposed to solve a constrained linear minimization problem. However,
due to very large $n$, computing the exact gradient $\nabla f(x^{(k)})$
can be very costly and it is interesting to solve the problem using
only approximates of it. It is possible to show that Franck-Wolfe
can converge even if the direction $s^{(k)}$ used at each iteration
is not exact, especially at the beginning of the procedure. 

Consequently, the new problem consists in being able to solve the
direction finding step using, for example, an online optimization
algorithm under noisy linear feedback. At round $k$, the algorithm will have to 
try various points $y_1,...,y_l$ and output $s^{(k)}$. We need to decide 
whether we want to impose a fixed confidence $\epsilon_k$ that control how far 
from the true optimal direction we can move, or whether we prefer to fix a 
number of exploration points so that it does take too long to make a decision. 

Remarks and questions : 
\begin{itemize}
	\item Maybe we can use regularity assumptions on $f$ in order to speed up 
	the learning between steps $k$ and $k+1$ : the gradient should not vary too 
	much...
	\item We are not in an adversarial case,
	\item In order to evaluate query point $y_i$, an estimate of the gradient 
	based on data samples is used to compute the scalar product $\left\langle 
	y_i,\nabla f(x^{(k)})\right\rangle$. It means that we only have noisy 
	feedback. Should the estimate vary along the exploration phase ? Should we 
	compute a new gradient estimate for each query point evaluation ? 
	\item The usual regret seems not to be the right metric as we don't mind 
	exploring wildly as long as the final decision is the right one. Simple 
	regret seems better. 
	\item In some special cases, say L1 or simplex constraints, the direction 
	we are looking for may take a finite number of values having only one 
	active coordinate at a time. 
	\item The literature on online optimization is quite rich, which would be 
	the closest papers ? 
\end{itemize}

\section{literature review}

\susection{Greedy methods, randomization approaches and multi-arm bandit algorithms for efficient sparsity-constrained optimization}

This is a journal paper on sparsity-constrained optimization that extends a previous conference version published at EUSIPCO 2015. It considers several algorithms (Matching Pursuit, Orthogonal Matching Pursuit and Frank-Wolfe) and shows that in the case of L1 regularization it is possible to speed up the direction finding step thanks to best-arm identification. In the FW algorithm, one needs to compute the gradient wihch is a matrix-vector product $X^T v$ where $X$ has dimension $n \times d$ ($d$ being potentially larger than $n$). However, because of the sparsity contraints, one expects the gradient to have only a few nonzero components among the $d$ available ones. Thus, the best-arm identification aims at finding efficiently those components. The authors suggest to use Successive halving (Karnin et al , ICML 2013) in a fixed-budget setting.

A few ideas that could improve on their results :
\begin{enumerate}
\item Because the required precision on the chosen gradient increases with the number of iterations, one would probably prefer a best-arm identification in a fixed-confidence setting so that the number of pulls is smaller in the first rounds and gets larger only when it is necessary;
\item The regularity hypotheses on the function imply that, depending on the size of the step size, the gradients at round $k$ and $k+1$ are close to each other. This means that, when looking for best arms, one should take into account the information of the previous iteration(s) somehow as a prior knowledge ($\approx$  tranfer learning ? );
\item As a remark, one may say that the problem of finding the gradient for L1 contrained problem is not that hard and already works quite well and quite fast without any bandit speeding so it may be necessary to justify the added value of our work, or even have another improvement on more difficult problems.
\end{enumerate}

\section{Open problem}


 
\end{document}
